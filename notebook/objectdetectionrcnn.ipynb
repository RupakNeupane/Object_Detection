{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1193394,"sourceType":"datasetVersion","datasetId":679312},{"sourceId":9514324,"sourceType":"datasetVersion","datasetId":5791730}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:07.746997Z","iopub.execute_input":"2024-09-30T09:25:07.747772Z","iopub.status.idle":"2024-09-30T09:25:07.764053Z","shell.execute_reply.started":"2024-09-30T09:25:07.747719Z","shell.execute_reply":"2024-09-30T09:25:07.763206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install selectivesearch torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:07.766088Z","iopub.execute_input":"2024-09-30T09:25:07.766494Z","iopub.status.idle":"2024-09-30T09:25:22.863552Z","shell.execute_reply.started":"2024-09-30T09:25:07.766451Z","shell.execute_reply":"2024-09-30T09:25:22.862394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Python in-built tools \nimport os\nimport pickle\nfrom tqdm import tqdm\n\n## Data Science Tools \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches \n%matplotlib inline\nimport torch\nimport torch.nn as nn\n\n## Image Processing Tools \nimport cv2\nimport selectivesearch\nfrom torch.utils.data import Dataset, DataLoader \n\n\n## Frameworks\nfrom torchvision import transforms\nfrom torchvision import models\nfrom torchsummary import summary\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:22.864951Z","iopub.execute_input":"2024-09-30T09:25:22.865324Z","iopub.status.idle":"2024-09-30T09:25:28.769287Z","shell.execute_reply.started":"2024-09-30T09:25:22.865288Z","shell.execute_reply":"2024-09-30T09:25:28.768192Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hyper Parameters","metadata":{}},{"cell_type":"code","source":"image_paths = r\"/kaggle/input/open-images-bus-trucks/images/images\"\ncsv_path = r\"/kaggle/input/open-images-bus-trucks/df.csv\"\nbatch_size = 2\nn_epochs = 5\nlearning_rate = 1e-4\nthreshold_iou = 0.3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:28.770882Z","iopub.execute_input":"2024-09-30T09:25:28.771405Z","iopub.status.idle":"2024-09-30T09:25:28.807328Z","shell.execute_reply.started":"2024-09-30T09:25:28.771363Z","shell.execute_reply":"2024-09-30T09:25:28.806383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# open image datasets loading","metadata":{}},{"cell_type":"code","source":"class OpenImageDataset(Dataset):\n\n    def __init__(self, image_paths, csv_path):\n        super().__init__()\n        self.image_paths = image_paths\n        self.csv_path = csv_path\n        self.df = pd.read_csv(csv_path)\n        self.unique_images = self.df['ImageID'].unique()\n\n    def __len__(self):\n        return len(self.unique_images)\n    \n    def __getitem__(self, index):\n        image_id = self.unique_images[index]\n        image_full_path = os.path.join(self.image_paths, image_id + \".jpg\")\n        image = cv2.imread(image_full_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        h, w, _ = image.shape\n        df = self.df.loc[self.df['ImageID'] == image_id]\n\n        bboxes = df[['XMin', 'YMin', 'XMax', 'YMax']].values\n        bboxes = (bboxes * np.array([w, h, w, h])).astype(np.uint16)\n\n        classes = df['LabelName'].values\n        return image, bboxes, classes, image_full_path\n        return image_full_path\n    \ndatasets = OpenImageDataset(image_paths, csv_path)\ndatasets[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:28.811780Z","iopub.execute_input":"2024-09-30T09:25:28.812089Z","iopub.status.idle":"2024-09-30T09:25:29.048974Z","shell.execute_reply.started":"2024-09-30T09:25:28.812057Z","shell.execute_reply":"2024-09-30T09:25:29.047897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(datasets)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.050429Z","iopub.execute_input":"2024-09-30T09:25:29.050844Z","iopub.status.idle":"2024-09-30T09:25:29.057578Z","shell.execute_reply.started":"2024-09-30T09:25:29.050797Z","shell.execute_reply":"2024-09-30T09:25:29.056708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, bbx, _, _ = datasets[14]\nplt.imshow(img)\nplt.axis('off')\n\nfor bb in bbx:\n    rect = patches.Rectangle(bb[:2], bb[2]-bb[0], bb[3]-bb[1], edgecolor = 'r', facecolor = 'none', linewidth = 1)\n    plt.gca().add_patch(rect)\nplt.show\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.058801Z","iopub.execute_input":"2024-09-30T09:25:29.059209Z","iopub.status.idle":"2024-09-30T09:25:29.352272Z","shell.execute_reply.started":"2024-09-30T09:25:29.059138Z","shell.execute_reply":"2024-09-30T09:25:29.351338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Utils Function","metadata":{}},{"cell_type":"code","source":"def extract_candidates(img):\n    _, regions = selectivesearch.selective_search(img, scale=4, min_size=20)\n    candidates = []\n    img_area = np.prod(img.shape[:2])\n    for region in regions:\n        if region['rect'] in candidates:\n            continue\n        if region['size'] < 0.05*img_area:\n            continue\n        if region['size'] > img_area:\n            continue \n        candidates.append(region['rect'])\n    return candidates\n\ndef extract_iou(bbox1, bbox2, epsilon=1e-5):\n    x1 = max(bbox1[0], bbox2[0])\n    y1 = max(bbox1[1], bbox2[1])\n    \n    x2 = min(bbox1[2], bbox2[2])\n    y2 = min(bbox1[3], bbox2[3])\n\n    width = x2 - x1\n    height = y2 - y1\n\n    if width < 0 or height < 0:\n        return 0\n    \n    intersection_area = width * height \n    area_1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n    area_2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n    union_area = area_1 + area_2 - intersection_area\n    return intersection_area / (union_area + epsilon)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.353537Z","iopub.execute_input":"2024-09-30T09:25:29.353855Z","iopub.status.idle":"2024-09-30T09:25:29.363885Z","shell.execute_reply.started":"2024-09-30T09:25:29.353821Z","shell.execute_reply":"2024-09-30T09:25:29.362967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FULL_PATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\n# # N = 100\n# for i, (image, bboxes, classes, image_full_path) in enumerate(datasets):\n#     # if i == N:\n#     #     break\n\n#     H, W, _ = image.shape\n#     candidates = extract_candidates(image)\n#     candidates = np.array([(x, y, x+w, y+h) for x, y, w, h in candidates])\n\n#     clss, deltas, rois = [], [], []\n#     ious = np.array([[extract_iou(candidate, bbox) for bbox in bboxes] for candidate in candidates])\n    \n#     for j, candidate in enumerate (candidates):\n#         cx, cy, cX, cY = candidate\n#         candidate_ious = ious[j]\n#         best_iou_at = np.argmax(candidate_ious)\n#         best_iou = candidate_ious[best_iou_at]\n#         best_bb = _x, _y, _X, _Y = bboxes[best_iou_at]\n\n#         if best_iou > threshold_iou:\n#             clss.append(classes[best_iou_at])\n#         else:\n#             clss.append('background')\n\n#         delta = np.array([_x - cx, _y - cy, _X - cX, _Y - cY]) / np.array([W, H, W, H])\n#         deltas.append(delta)\n\n#         rois.append(candidate / np.array([W, H, W, H]))\n\n#     FULL_PATHS.append(image_full_path)\n#     GTBBS.append(bboxes)\n#     CLSS.append(clss)\n#     DELTAS.append(deltas)\n#     ROIS.append(rois)\n#     IOUS.append(ious)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.365043Z","iopub.execute_input":"2024-09-30T09:25:29.365395Z","iopub.status.idle":"2024-09-30T09:25:29.374013Z","shell.execute_reply.started":"2024-09-30T09:25:29.365348Z","shell.execute_reply":"2024-09-30T09:25:29.373213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### multiprocessing mathi ko satta tala ko 2 ota section garyo vani mathi 4 hour ko kam 1 hour ma huncha","metadata":{}},{"cell_type":"code","source":"# def extract_all(t):\n#     image, bboxes, classes, image_full_path = t\n#     H, W, _ = image.shape\n#     candidates = extract_candidates(image)\n#     candidates = np.array([(x, y, x+w, y+h) for x,y,w,h in candidates])\n#     clss, deltas, rois = [], [], []\n#     ious = np.array([extract_iou(candidate, bbox) for bbox in bboxes] for candidate in candidates])\n#     for j, candidate in enumerate(candidates):\n#         cx, cy, CX, CY = candidate\n#         candidate_ious = ious[j]\n#         best_iou_at = np.argmax(candidate_ious)\n#         best_iou = candidate_ious[best_iou_at]\n#         best_bb = (_x, _y, _X, _Y) = bboxes[best_iou_at]\n        \n#         if best_iou > threshold_iou:\n#             clss.append(classes[best_iou_at])\n#         else:\n#             clss.append('background')\n        \n#         delta = np.array([_x - cx, _y - cy, _X - CX, _Y - CY]) / np.array([W, H, W, H])\n#         deltas.append(delta)\n#         rois.append(candidate / np.array([W, H, W, H]))\n    \n#     return image_full_path, bboxes, clss, deltas, rois, ious\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.375047Z","iopub.execute_input":"2024-09-30T09:25:29.375414Z","iopub.status.idle":"2024-09-30T09:25:29.387230Z","shell.execute_reply.started":"2024-09-30T09:25:29.375380Z","shell.execute_reply":"2024-09-30T09:25:29.386354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from multiprocessing import Pool\n# FULL_PATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\n\n# with Pool(14) as p:\n#     results = p.imap(extract_all, datasets)\n#     for result in tqdm(results):\n#         image_full_path, bboxes, clss, deltas, rois, ious = result\n#         FULL_PATHS.append(image_full_path)\n#         GTBBS.append(bboxes)\n#         CLSS.append(clss)\n#         DELTAS.append(deltas)\n#         ROIS.append(rois)\n#         IOUS.append(ious)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.388480Z","iopub.execute_input":"2024-09-30T09:25:29.388823Z","iopub.status.idle":"2024-09-30T09:25:29.401401Z","shell.execute_reply.started":"2024-09-30T09:25:29.388789Z","shell.execute_reply":"2024-09-30T09:25:29.400491Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pickle Utils Function","metadata":{}},{"cell_type":"code","source":"\ndef save_pickle(var, path):\n    with open(path, 'wb') as file:\n        pickle.dump(var, file)\n        \ndef load_pickle(path):\n    with open(path, 'rb') as file:\n        return pickle.load(file)\n        \n# save_pickle(FULL_PATHS, 'full_paths.pkl')\n\n\n# save_pickle(GTBBS, 'gtbbs.pkl')\n# save_pickle(CLSS, 'clss.pkl')   \n# save_pickle(DELTAS, 'deltas.pkl')\n# save_pickle(ROIS, 'rois.pkl')\n# save_pickle(IOUS, 'ious.pkl')\n\n# F_PATH = load_pickle('paths.pkl')\n# F_PATH","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.402476Z","iopub.execute_input":"2024-09-30T09:25:29.402756Z","iopub.status.idle":"2024-09-30T09:25:29.412621Z","shell.execute_reply.started":"2024-09-30T09:25:29.402724Z","shell.execute_reply":"2024-09-30T09:25:29.411826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Datasets for R-CNN","metadata":{}},{"cell_type":"code","source":"FULL_PATHS = load_pickle('/kaggle/input/pkl-files/fpath.pkl')\nGTBBS = load_pickle('/kaggle/input/pkl-files/gtbbs.pkl')\nCLSS = load_pickle('/kaggle/input/pkl-files/clss.pkl')\nDELTAS = load_pickle('/kaggle/input/pkl-files/deltas.pkl')\nROIS = load_pickle('/kaggle/input/pkl-files/rois.pkl')\nIOUS = load_pickle('/kaggle/input/pkl-files/ious.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:29.413687Z","iopub.execute_input":"2024-09-30T09:25:29.413990Z","iopub.status.idle":"2024-09-30T09:25:32.544924Z","shell.execute_reply.started":"2024-09-30T09:25:29.413959Z","shell.execute_reply":"2024-09-30T09:25:32.544128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_labels = np.unique(np.array([c for clss in CLSS for c in clss])) # 2d data lai 1d ma lerayera unique label nikaleko\ntarget2label = {i:label for i, label in enumerate(unique_labels)}\nlabel2target = {label:i for i, label in enumerate(unique_labels)}\n\nprint(target2label)\nprint(label2target)\nbackground_class = label2target['background']\nprint(background_class)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.548563Z","iopub.execute_input":"2024-09-30T09:25:32.548880Z","iopub.status.idle":"2024-09-30T09:25:32.771439Z","shell.execute_reply.started":"2024-09-30T09:25:32.548847Z","shell.execute_reply":"2024-09-30T09:25:32.770401Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(FULL_PATHS)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.772644Z","iopub.execute_input":"2024-09-30T09:25:32.773016Z","iopub.status.idle":"2024-09-30T09:25:32.779111Z","shell.execute_reply.started":"2024-09-30T09:25:32.772981Z","shell.execute_reply":"2024-09-30T09:25:32.778128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # sir ko code but error ayo\n\n# def preprocess(crop_img):\n#     crop_img = torch.tensor(crop_img).permute(2, 0, 1)\n#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n#     crop_img = normalize(crop_img)\n#     return crop_img.float()\n\n\n# class RCNNDataset(Dataset):\n#     def __init__(self, fpaths, rois, gtbbs, labels, deltas, ious):\n#         super().__init__()\n#         self.fpaths = fpaths\n#         self.rois = rois\n#         self.gtbbs = gtbbs\n#         self.labels = labels\n#         self.deltas = deltas\n#         self.ious = ious\n#         self.label2target = {'background':0, 'bus':1}\n\n#     def __len__(self):\n#         return len(self.fpaths)\n\n#     def __getitem__(self, index):\n#         fpath = self.fpaths[index]\n#         image = cv2.imread(fpath, cv2.IMREAD_COLOR)[..., ::-1]\n#         H, W, _ = image.shape\n        \n#         gtbbs = self.gtbbs[index]\n        \n#         rois = self.rois[index]\n#         bbs = (rois * np.array([W, H, W, H])).astype(np.uint8)\n        \n#         crops = [image[y:Y ,x:X] for x, y, X, Y in bbs]\n#         labels = self.labels[index]\n#         deltas = self.deltas[index]\n#         fpath = self.fpaths[index]\n        \n#         return image, gtbbs, bbs ,crops , labels, deltas, fpath\n\n        \n#     def collate_fn(self, batch):\n#         inputs, output_labels, output_deltas = [], [], []\n#         for i in range(len(batch)):\n#             image, gtbbs, bbs, crops, labels, deltas, fpath = batch[i]\n#             crops = [cv2.resize(crop, (224, 224)) for crop in crops]\n#             crops = [preprocess(crop/255.0)[None] for crop in crops]\n#             inputs.extend(crops)\n#             output_labels.extend([label2target[label] for label in labels])\n#             output_deltas.extend(deltas)\n            \n#     # yo tala ko 3 ota line le garda train garna lai feasible banako ho\n#         inputs = torch.cat(inputs).to(device)\n#         output_labels = torch.tensor(output_labels).long().to(device)\n#         output_deltas = torch.tensor(output_deltas).float().to(device)\n        \n#         return inputs, output_labels, output_deltas\n\n            \n            \n# # [] = [1,2,4] + [6, 7, 8]-> extend -> [1,2,4, 6, 7, 8]\n# # [] = [1,2,4] -> append -> [[1,2,4]]\n\n#         # Further processing (not defined yet)\n#         # You can add more code to preprocess inputs and handle labels, deltas, etc.\n\n# # n_train = len(FULL_PATHS) * 8//10\n\n# dataset = RCNNDataset(FULL_PATHS, GTBBS, CLSS, DELTAS , ROIS, IOUS)\n# # len(dataset)\n# dataset.collate_fn([dataset[10], dataset[11]])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.780417Z","iopub.execute_input":"2024-09-30T09:25:32.780777Z","iopub.status.idle":"2024-09-30T09:25:32.791539Z","shell.execute_reply.started":"2024-09-30T09:25:32.780743Z","shell.execute_reply":"2024-09-30T09:25:32.790730Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(crop_img):\n    crop_img = torch.tensor(crop_img).permute(2, 0, 1)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    crop_img = normalize(crop_img)\n    return crop_img.float()\n\n\nclass RCNNDataset(Dataset):\n    def __init__(self, fpaths, rois, gtbbs, labels, deltas, ious):\n        super().__init__()\n        self.fpaths = fpaths\n        self.rois = rois\n        self.gtbbs = gtbbs\n        self.labels = labels\n        self.deltas = deltas\n        self.ious = ious\n        self.label2target = {'background': 0, 'bus': 1}\n\n    def __len__(self):\n        return len(self.fpaths)\n\n    def __getitem__(self, index):\n        fpath = self.fpaths[index]\n        image = cv2.imread(fpath, cv2.IMREAD_COLOR)[..., ::-1]  # Convert BGR to RGB\n        H, W, _ = image.shape\n\n        gtbbs = self.gtbbs[index]\n\n        rois = self.rois[index]\n        bbs = (rois * np.array([W, H, W, H])).astype(np.uint8)  # Convert bounding boxes to integers\n\n        # bbs is required because the Selective Search algorithm may return bounding boxes outside the image\n        bbs = np.clip(bbs, 0, [W, H, W, H])\n\n        # Get crops and check if valid\n        # crops = []\n        # for x, y, X, Y in bbs:\n        #     if X > x and Y > y:  # Ensure bounding box has a valid area\n        #         crop = image[y:Y, x:X]\n        #         if crop.size > 0:  # Ensure the crop is not empty\n        #             crops.append(crop)\n        \n        crops = [image[y:Y, x:X] for x, y, X, Y in bbs]\n        labels = self.labels[index]\n        deltas = self.deltas[index]\n        \n        return image, gtbbs, bbs, crops, labels, deltas, fpath\n\n    def collate_fn(self, batch):\n        inputs, output_labels, output_deltas = [], [], []\n        for i in range(len(batch)):\n            image, gtbbs, bbs, crops, labels, deltas, fpath = batch[i]\n            \n            # Resize valid crops and preprocess them\n            crops = [cv2.resize(crop,(224, 224)) for crop in crops ]\n            crops = [preprocess(crop/255.0)[None] for crop in crops]\n            \n            inputs.extend(crops)\n            output_labels.extend([label2target[label] for label in labels])\n            output_deltas.extend(deltas)\n\n    # yo tala ko 3 ota line le garda train garna lai feasible banako ho\n        inputs = torch.cat(inputs).to(device)\n        output_labels = torch.tensor(output_labels).long().to(device)\n        output_deltas = torch.tensor(output_deltas).float().to(device)\n\n        return inputs, output_labels, output_deltas\n    \n\n\n# [] = [1,2,4] + [6, 7, 8]-> extend -> [1,2,4, 6, 7, 8]\n# [] = [1,2,4] -> append -> [[1,2,4]]\n\n        # Further processing (not defined yet)\n        # You can add more code to preprocess inputs and handle labels, deltas, etc.\n\nn_train = len(FULL_PATHS) * 8//10\ntrain_dataset = RCNNDataset(FULL_PATHS[:n_train], ROIS[:n_train], GTBBS[:n_train], CLSS[:n_train], DELTAS[:n_train], IOUS[:n_train])\ntest_dataset = RCNNDataset(FULL_PATHS[n_train:], ROIS[n_train:], GTBBS[n_train:], CLSS[n_train:], DELTAS[n_train:], IOUS[n_train:])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.792548Z","iopub.execute_input":"2024-09-30T09:25:32.792817Z","iopub.status.idle":"2024-09-30T09:25:32.812811Z","shell.execute_reply.started":"2024-09-30T09:25:32.792787Z","shell.execute_reply":"2024-09-30T09:25:32.811868Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"# Initialize DataLoader for the training and test datasets\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.814080Z","iopub.execute_input":"2024-09-30T09:25:32.814499Z","iopub.status.idle":"2024-09-30T09:25:32.827428Z","shell.execute_reply.started":"2024-09-30T09:25:32.814456Z","shell.execute_reply":"2024-09-30T09:25:32.826531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model architecture","metadata":{}},{"cell_type":"code","source":"backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\nbackbone.fc = nn.Sequential()\n\nfor param in backbone.parameters():\n    param.requires_grad = False\nbackbone.to(device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:32.828544Z","iopub.execute_input":"2024-09-30T09:25:32.828825Z","iopub.status.idle":"2024-09-30T09:25:34.763749Z","shell.execute_reply.started":"2024-09-30T09:25:32.828794Z","shell.execute_reply":"2024-09-30T09:25:34.762847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(backbone.to(device), (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:34.764828Z","iopub.execute_input":"2024-09-30T09:25:34.765118Z","iopub.status.idle":"2024-09-30T09:25:35.428174Z","shell.execute_reply.started":"2024-09-30T09:25:34.765085Z","shell.execute_reply":"2024-09-30T09:25:35.427219Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RCNN(nn.Module):\n    def __init__(self, backbone, n_classes):\n        super().__init__()\n        self.backbone = backbone\n        self.n_classes = n_classes\n        \n        self.classification_head = nn.Linear(2048,n_classes)\n\n        self.bbox_localization_head = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 4),\n            nn.Tanh()\n        )\n        \n        \n        \n        self.classification_loss = nn.CrossEntropyLoss()\n        self.localization_loss = nn.L1Loss()\n        \n        self.lmbda = 10.0 #priotizes localization loss over classification loss\n        \n        \n        \n    def forward(self, inputs):\n        feat = self.backbone(inputs)\n        \n        cls_score = self.classification_head(feat)\n        \n        deltas = self.bbox_localization_head(feat)\n        \n        return cls_score, deltas\n    \n    \n    def calculate_loss(self, _labels, _deltas, actual_labels, actual_deltas):\n        \n        #classification loss   \n        classification_loss = self.classification_loss(_labels, actual_labels)\n        \n        #localization loss\n        ix = torch.where(actual_labels != background_class)[0]\n        _deltas = _deltas[ix]\n        actual_deltas = actual_deltas[ix]\n        \n        if (len(ix)>0):\n            localization_loss = self.localization_loss(_deltas, actual_deltas)\n        else:\n            localization_loss = torch.tensor(0)\n            \n        total_loss = classification_loss + self.lmbda * localization_loss\n        \n        return total_loss, classification_loss, localization_loss\n            \n        \n# inputs, _ , targets_deltas = next(iter(train_dataloader))\n# rcnn = RCNN(backbone=backbone, n_classes=len(unique_labels)).to(device=device)\n# rcnn(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.429665Z","iopub.execute_input":"2024-09-30T09:25:35.430334Z","iopub.status.idle":"2024-09-30T09:25:35.440589Z","shell.execute_reply.started":"2024-09-30T09:25:35.430286Z","shell.execute_reply":"2024-09-30T09:25:35.439566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss Function\n# Train Batch\n# validate batch\n#validate batch\n#test prediction","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.441701Z","iopub.execute_input":"2024-09-30T09:25:35.441985Z","iopub.status.idle":"2024-09-30T09:25:35.455411Z","shell.execute_reply.started":"2024-09-30T09:25:35.441953Z","shell.execute_reply":"2024-09-30T09:25:35.454554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_batch(model, optimizer, inputs, actual_labels, deltas):\n    model.train()\n    optimizer.zero_grad()\n\n    # forward pass\n    _labels, _deltas = model(inputs)\n    total_loss, classification_loss, localization_loss = model.calculate_loss(_labels, _deltas, actual_labels, deltas)\n    conf, pred_labels = _labels.max(-1)\n    acc = pred_labels == actual_labels\n\n    # backward pass\n    total_loss.backward()\n    optimizer.step()\n\n    return _labels, _deltas, total_loss, classification_loss, localization_loss, acc\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.456516Z","iopub.execute_input":"2024-09-30T09:25:35.456847Z","iopub.status.idle":"2024-09-30T09:25:35.465905Z","shell.execute_reply.started":"2024-09-30T09:25:35.456812Z","shell.execute_reply":"2024-09-30T09:25:35.464994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad\ndef validate_batch(model, inputs, actual_labels, deltas):\n    model.eval()\n    _labels, _deltas = model(inputs)\n    total_loss, classification_loss, localization_loss = model.calculate_loss(_labels, _deltas, actual_labels, deltas)\n\n    conf, pred_labels = _labels.max(-1)\n    acc = pred_labels == actual_labels\n\n    return _labels, _deltas, total_loss, classification_loss, localization_loss, acc\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.467125Z","iopub.execute_input":"2024-09-30T09:25:35.467806Z","iopub.status.idle":"2024-09-30T09:25:35.479192Z","shell.execute_reply.started":"2024-09-30T09:25:35.467759Z","shell.execute_reply":"2024-09-30T09:25:35.478410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rcnn = RCNN(backbone, 3).to(device=device)\noptimizer = torch.optim.SGD(rcnn.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.480393Z","iopub.execute_input":"2024-09-30T09:25:35.480755Z","iopub.status.idle":"2024-09-30T09:25:35.501697Z","shell.execute_reply.started":"2024-09-30T09:25:35.480713Z","shell.execute_reply":"2024-09-30T09:25:35.500891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training pipeline\ntrain_history = {\n    'total_loss': [],\n    'detection_loss': [],\n    'localization_loss': [],\n    'accuracy': []\n}\n\ntest_history = {\n    'total_loss': [],\n    'detection_loss': [],\n    'localization_loss': [],\n    'accuracy': []\n}\n\nfor epoch in range(1, n_epochs + 1):\n    epoch_train_total_loss = 0\n    epoch_train_detection_loss = 0\n    epoch_train_localization_loss = 0\n    epoch_train_acc = []\n\n    for inputs, labels, deltas in tqdm(train_dataloader, desc=f'Training {epoch} of {n_epochs}'):\n        _inputs ,_deltas, total_loss , classification_loss, localization_loss, acc = train_batch(rcnn, optimizer, inputs, labels, deltas)\n        epoch_train_total_loss += total_loss.item()\n        epoch_train_detection_loss += classification_loss.item()\n        epoch_train_localization_loss += localization_loss.item()\n        epoch_train_acc.extend(acc.tolist())\n        \n    epoch_train_total_loss /= len(train_dataloader)\n    epoch_train_detection_loss /= len(train_dataloader)\n    epoch_train_localization_loss /= len(train_dataloader)\n    epoch_train_acc = sum(epoch_train_acc)  / len(epoch_train_acc)\n        \n    epoch_test_total_loss = 0\n    epoch_test_detection_loss = 0\n    epoch_test_localization_loss = 0\n    epoch_test_acc = []\n\n    for inputs, labels, deltas in tqdm(test_dataloader, desc=f'Testing '):\n        _inputs ,_deltas, total_loss ,classification_loss, localization_loss, acc = validate_batch(rcnn, inputs, labels, deltas)\n        epoch_test_total_loss += total_loss.item()\n        epoch_test_detection_loss += classification_loss.item()\n        epoch_test_localization_loss += localization_loss.item()\n        epoch_test_acc.extend(acc.tolist())\n        \n    epoch_test_total_loss /= len(test_dataloader)   \n    epoch_test_detection_loss /= len(test_dataloader)\n    epoch_test_localization_loss /= len(test_dataloader)\n    epoch_test_acc = sum(epoch_test_acc) / len(test_dataloader)\n\n    train_history.get('total_loss').append(epoch_train_total_loss)\n    train_history.get('detection_loss').append(epoch_train_detection_loss)\n    train_history.get('localization_loss').append(epoch_train_localization_loss)\n    train_history.get('accuracy').append(epoch_train_acc)\n    \n    test_history.get('total_loss').append(epoch_test_total_loss)\n    test_history.get('detection_loss').append(epoch_test_detection_loss)\n    test_history.get('localization_loss').append(epoch_test_localization_loss)\n    test_history.get('accuracy').append(epoch_test_acc)\n    \n    print(f'Epoch {epoch} of {n_epochs}, Training_loss: {epoch_train_total_loss}, Testing Detection Loss: {epoch_test_total_loss}, Testing Localization Loss: {epoch_test_localization_loss}, Testing Accuracy: {epoch_test_acc}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:25:35.503111Z","iopub.execute_input":"2024-09-30T09:25:35.503540Z","iopub.status.idle":"2024-09-30T09:26:35.258254Z","shell.execute_reply.started":"2024-09-30T09:25:35.503498Z","shell.execute_reply":"2024-09-30T09:26:35.256983Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(rcnn,'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:26:35.259191Z","iopub.status.idle":"2024-09-30T09:26:35.259588Z","shell.execute_reply.started":"2024-09-30T09:26:35.259377Z","shell.execute_reply":"2024-09-30T09:26:35.259395Z"},"trusted":true},"outputs":[],"execution_count":null}]}